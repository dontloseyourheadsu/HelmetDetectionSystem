{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup & Imports ---\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import shutil\n",
    "import pathlib\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import gdown\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducible results\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850d6f8",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "Downloads the dataset from Google Drive and extracts frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATASET_LINK = \"https://drive.google.com/drive/folders/1gLRc8noJhQjpkD0F6hnvUO92qi5YrbXH?usp=drive_link\"\n",
    "ROOT = pathlib.Path(\".\").resolve()\n",
    "DOWNLOAD_DIR = ROOT / \"dataset\" / \"download\"\n",
    "FRAMES_DIR = ROOT / \"dataset\" / \"frames\"\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"}\n",
    "CLASS_KEYS = [\"sterile\", \"hair\", \"trash\", \"both\"]\n",
    "\n",
    "def slugify(stem: str) -> str:\n",
    "    s = stem.lower().strip().replace(\" \", \"-\").replace(\"_\", \"-\")\n",
    "    s = \"\".join(ch for ch in s if ch.isalnum() or ch == \"-\")\n",
    "    return s or \"video\"\n",
    "\n",
    "def download_dataset(force: bool = False) -> None:\n",
    "    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    existing = [p for p in DOWNLOAD_DIR.rglob(\"*\") if p.suffix.lower() in VIDEO_EXTS]\n",
    "    if existing and not force:\n",
    "        print(f\"Found {len(existing)} local video(s); skipping download.\")\n",
    "        return\n",
    "    print(f\"Downloading dataset into: {DOWNLOAD_DIR}\")\n",
    "    try:\n",
    "        gdown.download_folder(DATASET_LINK, output=str(DOWNLOAD_DIR), quiet=False, use_cookies=False)\n",
    "    except Exception:\n",
    "        gdown.download(DATASET_LINK, output=str(DOWNLOAD_DIR), quiet=False)\n",
    "\n",
    "def extract_frames(video_path: Path, target_count: int, out_dir: Path) -> int:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened() or target_count <= 0: return 0\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    if total <= 0: cap.release(); return 0\n",
    "    \n",
    "    desired = min(target_count, total)\n",
    "    indices = np.linspace(0, total - 1, num=desired, dtype=np.int64)\n",
    "    saved = 0\n",
    "    for fidx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(fidx))\n",
    "        ret, frame = cap.read()\n",
    "        if ret and frame is not None:\n",
    "            cv2.imwrite(str(out_dir / f\"frame_{saved:05d}.png\"), frame)\n",
    "            saved += 1\n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "# --- Run Dataset Builder ---\n",
    "# Uncomment to run download and extraction\n",
    "# download_dataset()\n",
    "# videos = sorted([p for p in DOWNLOAD_DIR.rglob(\"*\") if p.suffix.lower() in VIDEO_EXTS])\n",
    "# for v in videos:\n",
    "#     out = FRAMES_DIR / slugify(v.stem)\n",
    "#     extract_frames(v, 1000, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b1a37",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "Applies morphological filters (TopHat) and contrast enhancement to highlight hair and crumbs while removing large noise (logos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ff4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_large_noise(tophat_image, threshold_value=15, max_area=80):\n",
    "    _, binary = cv2.threshold(tophat_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    clean_mask = np.zeros_like(tophat_image)\n",
    "    for cnt in contours:\n",
    "        if 0 < cv2.contourArea(cnt) < max_area:\n",
    "            cv2.drawContours(clean_mask, [cnt], -1, 255, -1)\n",
    "    return cv2.bitwise_and(tophat_image, tophat_image, mask=clean_mask)\n",
    "\n",
    "def morphological_contrast_enhancement(image, kernel_size=19, crumb_boost=4.0, hair_boost=4.0, shadow_gamma=0.6):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    smoothed = cv2.bilateralFilter(gray, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    norm_img = smoothed.astype(np.float32) / 255.0\n",
    "    lifted = np.power(norm_img, shadow_gamma)\n",
    "    lifted_uint8 = (lifted * 255).astype(np.uint8)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    white_tophat = cv2.morphologyEx(lifted_uint8, cv2.MORPH_TOPHAT, kernel)\n",
    "    white_tophat_clean = remove_large_noise(white_tophat, threshold_value=10, max_area=60)\n",
    "    black_tophat = cv2.morphologyEx(lifted_uint8, cv2.MORPH_BLACKHAT, kernel)\n",
    "    \n",
    "    flat_background = np.full_like(lifted_uint8, 128, dtype=np.float32)\n",
    "    result = flat_background + (white_tophat_clean.astype(np.float32) * crumb_boost)\n",
    "    result -= (black_tophat.astype(np.float32) * hair_boost)\n",
    "    \n",
    "    result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
    "    return clahe.apply(result)\n",
    "\n",
    "def process_images(input_dir, output_dir, num_images_per_folder=1000):\n",
    "    if os.path.exists(output_dir): shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    subfolders = [f for f in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, f))]\n",
    "    if not subfolders: subfolders = ['.']\n",
    "    \n",
    "    for folder in subfolders:\n",
    "        in_path = os.path.join(input_dir, folder)\n",
    "        out_path = os.path.join(output_dir, folder)\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "        \n",
    "        files = sorted([f for f in os.listdir(in_path) if f.lower().endswith(('.png', '.jpg'))])[:num_images_per_folder]\n",
    "        for f in tqdm(files, desc=f\"Processing {folder}\"):\n",
    "            img = cv2.imread(os.path.join(in_path, f))\n",
    "            if img is not None:\n",
    "                res = morphological_contrast_enhancement(img)\n",
    "                cv2.imwrite(os.path.join(out_path, f\"{os.path.splitext(f)[0]}_filtered.png\"), res)\n",
    "\n",
    "# --- Run Preprocessing ---\n",
    "# process_images('dataset/frames', 'filtering/processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c22b2",
   "metadata": {},
   "source": [
    "## 3. Autoencoder (Anomaly Detection)\n",
    "Trains a Convolutional Autoencoder on clean images. Anomalies are detected based on reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b11693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Autoencoder Configuration ---\n",
    "AE_BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)\n",
    "AE_EPOCHS = 20\n",
    "\n",
    "data_dir = pathlib.Path(\"filtering/processed\")\n",
    "clean_dir = data_dir / \"clean\"\n",
    "hair_dir = data_dir / \"hair\"\n",
    "trash_dir = data_dir / \"trash\"\n",
    "trash_hair_dir = data_dir / \"trash-hair\"\n",
    "\n",
    "def load_image_ae(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=1)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def get_files(directory, limit=None):\n",
    "    if not directory.exists(): return []\n",
    "    files = sorted([str(p) for p in directory.glob(\"*\") if p.suffix.lower() in ['.png', '.jpg']])\n",
    "    return files[:limit] if limit else files\n",
    "\n",
    "# Prepare Datasets\n",
    "all_clean = get_files(clean_dir)\n",
    "test_clean_paths = all_clean[:100]\n",
    "train_clean_paths = all_clean[100:]\n",
    "\n",
    "train_ds_ae = tf.data.Dataset.from_tensor_slices(train_clean_paths)\n",
    "train_ds_ae = train_ds_ae.map(load_image_ae).map(lambda x: (x, x))\n",
    "train_ds_ae = train_ds_ae.batch(AE_BATCH_SIZE).cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build Model\n",
    "class ConvolutionalAutoencoder(models.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(128, 128, 1)),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(128, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')\n",
    "        ])\n",
    "    def call(self, x): return self.decoder(self.encoder(x))\n",
    "\n",
    "autoencoder = ConvolutionalAutoencoder()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train\n",
    "# history_ae = autoencoder.fit(train_ds_ae, epochs=AE_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472adc5",
   "metadata": {},
   "source": [
    "## 4. CNN Classifier\n",
    "Trains a supervised CNN to classify images into 4 categories: Clean, Hair, Trash, Trash-Hair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classifier Configuration ---\n",
    "CLS_BATCH_SIZE = 32\n",
    "CLS_EPOCHS = 15\n",
    "\n",
    "print(\"Loading Classifier Data...\")\n",
    "# Note: Ensure 'filtering/processed' exists and has subfolders\n",
    "if data_dir.exists():\n",
    "    train_ds_cls = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir, validation_split=0.2, subset=\"training\", seed=123,\n",
    "        color_mode='grayscale', image_size=IMG_SIZE, batch_size=CLS_BATCH_SIZE\n",
    "    )\n",
    "    val_ds_cls = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir, validation_split=0.2, subset=\"validation\", seed=123,\n",
    "        color_mode='grayscale', image_size=IMG_SIZE, batch_size=CLS_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    class_names = train_ds_cls.class_names\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    \n",
    "    train_ds_cls = train_ds_cls.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds_cls = val_ds_cls.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Build Model\n",
    "    classifier = models.Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(128, 128, 1)),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(len(class_names))\n",
    "    ])\n",
    "\n",
    "    classifier.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    # history_cls = classifier.fit(train_ds_cls, validation_data=val_ds_cls, epochs=CLS_EPOCHS)\n",
    "else:\n",
    "    print(\"Data directory not found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed60368",
   "metadata": {},
   "source": [
    "## 5. Roboflow Workflow\n",
    "The Roboflow workflow `find-white-dots-and-thin-black-lines` uses a SAM-based approach to detect:\n",
    "- **White dots** (Trash/Crumbs)\n",
    "- **Thin black lines** (Hair)\n",
    "\n",
    "### Evaluation Results\n",
    "The following results were obtained by running the workflow on the last 100 images of each class:\n",
    "\n",
    "| Class | Avg White Dots | Avg Black Lines |\n",
    "| :--- | :--- | :--- |\n",
    "| **Clean** | 0.00 | 1.13 |\n",
    "| **Hair** | 0.00 | 0.03 |\n",
    "| **Trash** | 1.65 | 0.65 |\n",
    "| **Trash-Hair** | 0.00 | 1.70 |\n",
    "\n",
    "**Observations:**\n",
    "- **Trash Detection**: The model successfully detects \"White dots\" in the Trash class (Avg 1.65) compared to Clean/Hair (0.00).\n",
    "- **Hair Detection**: The \"Thin black lines\" detection is inconsistent. While \"Trash-Hair\" shows the highest count (1.70), the pure \"Hair\" class shows very few (0.03), and surprisingly, \"Clean\" images show some false positives (1.13). This suggests the \"Thin black lines\" prompt or model might need refinement to distinguish actual hair from background artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install Roboflow Dependencies ---\n",
    "# Uncomment the line below to install the required packages in Colab or your local environment\n",
    "# !pip install inference-sdk python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Roboflow Evaluation Script ---\n",
    "import os\n",
    "import glob\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables (create a .env file with ROBOFLOW_API_KEY=...)\n",
    "load_dotenv()\n",
    "\n",
    "# You can set your API key here directly if not using .env\n",
    "# os.environ[\"ROBOFLOW_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "api_key = os.getenv(\"ROBOFLOW_INFERENCE_API_KEY\") or os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"⚠️ ROBOFLOW_API_KEY not found. Please set it in a .env file or environment variables to run this cell.\")\n",
    "else:\n",
    "    try:\n",
    "        client = InferenceHTTPClient(\n",
    "            api_url=\"https://serverless.roboflow.com\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "\n",
    "        # Configuration\n",
    "        WORKSPACE_NAME = \"dontloseyourheadsu\"\n",
    "        WORKFLOW_ID = \"find-white-dots-and-thin-black-lines\"\n",
    "        CLASSES = [\"clean\", \"hair\", \"trash\", \"trash-hair\"]\n",
    "        \n",
    "        # Adjust path for Notebook environment\n",
    "        BASE_DIR = pathlib.Path(\"filtering/processed\")\n",
    "        LIMIT = 100\n",
    "\n",
    "        results_summary = {}\n",
    "\n",
    "        print(f\"Starting evaluation on last {LIMIT} images per class...\")\n",
    "\n",
    "        for class_name in CLASSES:\n",
    "            class_dir = BASE_DIR / class_name\n",
    "            if not class_dir.exists():\n",
    "                print(f\"Directory not found: {class_dir}\")\n",
    "                continue\n",
    "\n",
    "            # Get all images, sort them to get the 'last' ones consistently\n",
    "            images = sorted(list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")))\n",
    "            \n",
    "            if not images:\n",
    "                print(f\"No images found for {class_name}\")\n",
    "                continue\n",
    "                \n",
    "            # Take last N images\n",
    "            test_images = images[-LIMIT:] if len(images) > LIMIT else images\n",
    "            print(f\"Processing {len(test_images)} images for class '{class_name}'...\")\n",
    "            \n",
    "            white_dots_counts = []\n",
    "            black_lines_counts = []\n",
    "            \n",
    "            for img_path in tqdm(test_images):\n",
    "                try:\n",
    "                    # Run workflow\n",
    "                    result = client.run_workflow(\n",
    "                        workspace_name=WORKSPACE_NAME,\n",
    "                        workflow_id=WORKFLOW_ID,\n",
    "                        images={\"image\": str(img_path)},\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                    \n",
    "                    # Parse results\n",
    "                    predictions = []\n",
    "                    if isinstance(result, list):\n",
    "                        result = result[0]\n",
    "                    \n",
    "                    # Look for prediction lists in the result dictionary\n",
    "                    for key, value in result.items():\n",
    "                        if isinstance(value, dict) and 'predictions' in value:\n",
    "                            predictions.extend(value['predictions'])\n",
    "                        elif isinstance(value, list):\n",
    "                            if value and isinstance(value[0], dict) and 'class' in value[0]:\n",
    "                                predictions.extend(value)\n",
    "\n",
    "                    wd_count = 0\n",
    "                    bl_count = 0\n",
    "                    \n",
    "                    for pred in predictions:\n",
    "                        if 'class' in pred:\n",
    "                            label = pred['class']\n",
    "                            if \"White dots\" in label or \"white dots\" in label:\n",
    "                                wd_count += 1\n",
    "                            elif \"thin black lines\" in label or \"black lines\" in label:\n",
    "                                bl_count += 1\n",
    "                    \n",
    "                    white_dots_counts.append(wd_count)\n",
    "                    black_lines_counts.append(bl_count)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path.name}: {e}\")\n",
    "\n",
    "            # Calculate stats\n",
    "            avg_wd = statistics.mean(white_dots_counts) if white_dots_counts else 0\n",
    "            avg_bl = statistics.mean(black_lines_counts) if black_lines_counts else 0\n",
    "            \n",
    "            results_summary[class_name] = {\n",
    "                \"avg_white_dots\": avg_wd,\n",
    "                \"avg_black_lines\": avg_bl\n",
    "            }\n",
    "\n",
    "        print(\"\\n--- Roboflow SAM Evaluation Results ---\")\n",
    "        for cls, stats in results_summary.items():\n",
    "            print(f\"Class: {cls}\")\n",
    "            print(f\"  Avg White Dots: {stats['avg_white_dots']:.2f}\")\n",
    "            print(f\"  Avg Black Lines: {stats['avg_black_lines']:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during initialization: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
